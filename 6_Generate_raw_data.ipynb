{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conducting the final study by changing the combination of the data from the eron dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gouravsanjuktabhabesh/.conda/envs/ml_101_env/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import pandas as pd\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the input from the code file 5 where we extracted the raw mails from the raw data supplied \n",
    "df = pd.read_csv(\"Res/accepted_raw_data/filtered_raw_data.csv\",sep=\",\")\n",
    "df.rename(columns={\"content_mail\":\"content_entire_mail\",\"filepath\":\"file_name\",\"owner\":\"owner_mail\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Functions to extract the messgaes from the mails supplied in the raw mails\n",
    "def html_extract(html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "    text = soup.get_text()\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    return text\n",
    "def extract_content(entire_mail):\n",
    "    if(\"<html\" in entire_mail.lower() and \"</html>\" in entire_mail.lower()):\n",
    "        entire_mail_lower = entire_mail.lower().replace(\"<html>\",\"\\n<html>\\n\").replace(\"</html>\",\"\\n</html>\\n\")\n",
    "        a = entire_mail_lower.split(\"\\n\")\n",
    "        actual = entire_mail.split(\"\\n\")\n",
    "        try:\n",
    "            start = a.index(filter(lambda x: x.startswith('<html'), a)[0])\n",
    "            end = a.index(\"</html>\")+1\n",
    "            html = \"\\n\".join(actual[start:end])\n",
    "        except:\n",
    "            html = entire_mail[entire_mail_lower.find(\"<html\"):entire_mail_lower.rfind(\"</html>\")]+\"</html>\"\n",
    "        return html_extract(html)\n",
    "    elif(\"Content-Type: text/plain\" in entire_mail):\n",
    "        try:\n",
    "            a = entire_mail.split(\"Content-Type: text/plain\")[-1].split(\"\\r\\n\")\n",
    "            return a[a.index(\"\")+1]\n",
    "        except:\n",
    "            try:\n",
    "                a = entire_mail.split(\"Content-Type: text/plain\")[-1].split(\"\\n\")\n",
    "                return a[a.index(\"\")+1]\n",
    "            except:\n",
    "                return \"False\"\n",
    "    else:\n",
    "        try:\n",
    "            html = \"<html>\"+entire_mail[entire_mail.lower().find(\"<body\"):entire_mail.lower().rfind(\"</body>\")]+\"</body></html>\"\n",
    "            return html_extract(html)\n",
    "        except:\n",
    "            return \"False\"\n",
    "df[\"content_message\"] = df.apply(lambda row: extract_content(row[\"content_entire_mail\"]),axis=1)\n",
    "df = df.loc[df[\"content_message\"]!=False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>content_entire_mail</th>\n",
       "      <th>content_message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>owner_mail</th>\n",
       "      <th>mail_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BG</th>\n",
       "      <th>spam</th>\n",
       "      <td>1664</td>\n",
       "      <td>1664</td>\n",
       "      <td>1664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GP</th>\n",
       "      <th>spam</th>\n",
       "      <td>13703</td>\n",
       "      <td>13703</td>\n",
       "      <td>13703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SH</th>\n",
       "      <th>spam</th>\n",
       "      <td>7780</td>\n",
       "      <td>7780</td>\n",
       "      <td>7780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beck-s</th>\n",
       "      <th>ham</th>\n",
       "      <td>1966</td>\n",
       "      <td>1966</td>\n",
       "      <td>1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>farmer-d</th>\n",
       "      <th>ham</th>\n",
       "      <td>3669</td>\n",
       "      <td>3669</td>\n",
       "      <td>3669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kaminski-v</th>\n",
       "      <th>ham</th>\n",
       "      <td>4363</td>\n",
       "      <td>4363</td>\n",
       "      <td>4363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kitchen-l</th>\n",
       "      <th>ham</th>\n",
       "      <td>4012</td>\n",
       "      <td>4012</td>\n",
       "      <td>4012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lokay-m</th>\n",
       "      <th>ham</th>\n",
       "      <td>2364</td>\n",
       "      <td>2364</td>\n",
       "      <td>2364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>williams-w3</th>\n",
       "      <th>ham</th>\n",
       "      <td>2714</td>\n",
       "      <td>2714</td>\n",
       "      <td>2714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        file_name  content_entire_mail  content_message\n",
       "owner_mail  mail_label                                                 \n",
       "BG          spam             1664                 1664             1664\n",
       "GP          spam            13703                13703            13703\n",
       "SH          spam             7780                 7780             7780\n",
       "beck-s      ham              1966                 1966             1966\n",
       "farmer-d    ham              3669                 3669             3669\n",
       "kaminski-v  ham              4363                 4363             4363\n",
       "kitchen-l   ham              4012                 4012             4012\n",
       "lokay-m     ham              2364                 2364             2364\n",
       "williams-w3 ham              2714                 2714             2714"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stats after cleaning and extracting the data from the enron raw data\n",
    "df.groupby([\"owner_mail\",\"mail_label\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the same features that we created for the enron 1 to 6 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of lines including the subject line\n",
    "df[\"line_count\"] = df.apply(lambda row: row[\"content_message\"].replace(\"\\r\\n\",\"\\n\")\n",
    "                            .replace(\"\\n\\n\",\"\\n\").count(\"\\n\"),axis=1)\n",
    "\n",
    "# Extracting the tokens out of the messages in a column called \"tokens\"\n",
    "df[\"tokens\"] = df.apply(lambda row: row[\"content_message\"]\\\n",
    "    .replace(\"\\\\r\\\\n\",\" \").replace(\"\\\\n\\\\n\",\" \").replace(\".\",\" . \").replace(\",\",\" , \").replace(\";\",\" ; \")\\\n",
    "    .replace(\":\",\" : \").replace(\"!\",\" ! \").replace(\"?\",\" ? \").replace(\"$\",\" $ \")\\\n",
    "    .replace(\"-\",\" - \").replace(\"=\",\" = \").replace(\"&\",\" & \").replace(\"/\",\" / \").split(),axis=1)\n",
    "\n",
    "# Creating a column to count the total number of tokens\n",
    "df[\"token_count\"] = df.apply(lambda row: len(row[\"tokens\"]),axis=1)\n",
    "\n",
    "# Creating a column to count the number of punctuations\n",
    "df[\"punctuations_count\"] = df.apply(lambda row: sum([True if x in string.punctuation else False for x in row[\"tokens\"]]),axis=1)\n",
    "\n",
    "# Single character count in the list of tokens\n",
    "df[\"single_char_count\"] = df.apply(lambda row: sum([True if (len(x)==1 and x not in string.punctuation) else False for x in row[\"tokens\"]]),axis=1)\n",
    "\n",
    "# Column to count the tokens that are numbers \n",
    "df[\"number_token_count\"] = df.apply(lambda row: sum([True if x.isdigit() else False for x in row[\"tokens\"]]),axis=1)\n",
    "\n",
    "# Tokens that are years mentioned in between 1970 to 2050\n",
    "df[\"year_count\"] = df.apply(lambda row: sum([True if (x >= \"1970\" and x <= \"2050\") else False for x in row[\"tokens\"]]),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gouravsanjuktabhabesh/.conda/envs/ml_101_env/lib/python2.7/site-packages/numpy/lib/nanfunctions.py:1019: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/gouravsanjuktabhabesh/.conda/envs/ml_101_env/lib/python2.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: Mean of empty slice\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "NOT_rareword=[]\n",
    "f = open('Res/NOT_rareword.txt', 'r')\n",
    "for word in f:\n",
    "    NOT_rareword.append(word.split(\"\\n\")[0])\n",
    "\n",
    "# Number of stopwords count in spam messages\n",
    "df[\"stopword_count\"] = df.apply(lambda row: np.sum([True if x in stop_words else False \\\n",
    "                                                    for x in row[\"tokens\"]]),axis=1)\n",
    "\n",
    "# Number of useful words in the spam messages\n",
    "df[\"useful_tokens\"] = df.apply(lambda row: filter(lambda x: x not in stop_words and \\\n",
    "                              x not in string.punctuation and \\\n",
    "                              x.isdigit()==False, row[\"tokens\"]),axis=1)\n",
    "\n",
    "# Median length of useful tokens in the spam messages\n",
    "df[\"median_useful_token_len\"] = df.apply(lambda row: np.nanmedian([len(x) if len(x)!=1 else np.nan for x in row[\"useful_tokens\"]]),axis=1)\n",
    "\n",
    "# Average length of useful tokens in the spam messages\n",
    "df[\"avg_useful_token_len\"] = df\\\n",
    "    .apply(lambda row: np.nanmean([len(x) if len(x)!=1 else np.nan for x in row[\"useful_tokens\"]]),axis=1)\n",
    "\n",
    "# Extracting the rarewords in spam messages and storing them in a new column\n",
    "df[\"rareword_count\"] = df\\\n",
    "    .apply(lambda row: np.sum([True if x not in NOT_rareword else False for x in set(row[\"useful_tokens\"])]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and Lemmatization of the useful tokens\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "df[\"stemmed_tokens\"] = df.apply(lambda row: [porter.stem(word) \n",
    "                                             for word in filter(lambda x: len(x)>2 and \n",
    "                                                                is_ascii(x),row[\"useful_tokens\"])],axis=1)\n",
    "\n",
    "df[\"lemma_tokens\"] = df.apply(lambda row: [wordnet_lemmatizer.lemmatize(word) \n",
    "                                           for word in filter(lambda x: len(x)>2 and \n",
    "                                                              is_ascii(x),row[\"useful_tokens\"])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_tokens_dict = eval(open('Res/useful_tokens_dict.txt', 'r').read())\n",
    "def intersect_useful_tokens(lemma_tokens_row):\n",
    "    return (list(set(map(lambda x: x.lower(), lemma_tokens_row))&set(useful_tokens_dict.keys())))\n",
    "df[\"attributes\"] = df.apply(lambda row: intersect_useful_tokens(row[\"lemma_tokens\"]),axis=1)\n",
    "df[\"attributes_len\"] = df.apply(lambda row: float(len(row[\"attributes\"])),axis=1)\n",
    "df[\"corpus\"] = df.apply(lambda row: \" \".join(row[\"attributes\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the tf-idf data frame\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = list(df[\"corpus\"])\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "attributes_df = pd.DataFrame(X.todense(),columns = vectorizer.get_feature_names())\n",
    "df = pd.concat([df,attributes_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns \"content_entire_mail\",\"content_message\" and writing the complete dataset into complete_data_attributes_df.csv\n",
    "df1 = df.drop([\"content_entire_mail\",\"content_message\"],axis=1)\n",
    "df1.to_csv(\"Res/Complete_data_processed/complete_data_attributes_df.csv\",sep=\",\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the new Enron datasets 7 to 12 from the raw data\n",
    "enron7 = pd.concat([df.loc[df[\"owner_mail\"]==\"farmer-d\"],df.loc[df[\"owner_mail\"]==\"BG\"].sample(n=1500,random_state=7)])\n",
    "enron8 = pd.concat([df.loc[df[\"owner_mail\"]==\"kaminski-v\"],df.loc[df[\"owner_mail\"]==\"GP\"].sample(n=1500,random_state=8)])\n",
    "enron9 = pd.concat([df.loc[df[\"owner_mail\"]==\"lokay-m\"],df.loc[df[\"owner_mail\"]==\"SH\"].sample(n=1500,random_state=9)])\n",
    "enron10 = pd.concat([df.loc[df[\"owner_mail\"]==\"williams-w3\"].sample(n=1500,random_state=10),df.loc[df[\"owner_mail\"]==\"BG\"]])\n",
    "enron11 = pd.concat([df.loc[df[\"owner_mail\"]==\"beck-s\"].sample(n=1500,random_state=11),df.loc[df[\"owner_mail\"]==\"GP\"].sample(n=4500,random_state=11)])\n",
    "enron12 = pd.concat([df.loc[df[\"owner_mail\"]==\"kitchen-l\"].sample(n=1500,random_state=12),df.loc[df[\"owner_mail\"]==\"SH\"].sample(n=4500,random_state=12)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing those mails that have no text inside them\n",
    "enron7.drop(enron7.index[np.where(np.isnan(enron7[[\"median_useful_token_len\"]]))[0]],inplace=True)\n",
    "enron8.drop(enron8.index[np.where(np.isnan(enron8[[\"median_useful_token_len\"]]))[0]],inplace=True)\n",
    "enron9.drop(enron9.index[np.where(np.isnan(enron9[[\"median_useful_token_len\"]]))[0]],inplace=True)\n",
    "enron10.drop(enron10.index[np.where(np.isnan(enron10[[\"median_useful_token_len\"]]))[0]],inplace=True)\n",
    "enron11.drop(enron11.index[np.where(np.isnan(enron11[[\"median_useful_token_len\"]]))[0]],inplace=True)\n",
    "enron12.drop(enron12.index[np.where(np.isnan(enron12[[\"median_useful_token_len\"]]))[0]],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the individual files enron 7 to 12 to respective file names csv\n",
    "enron7.drop([\"content_entire_mail\",\"content_message\"],axis=1).to_csv(\"Res/Complete_data_processed/enron7.csv\",sep=\",\",index=False)\n",
    "enron8.drop([\"content_entire_mail\",\"content_message\"],axis=1).to_csv(\"Res/Complete_data_processed/enron8.csv\",sep=\",\",index=False)\n",
    "enron9.drop([\"content_entire_mail\",\"content_message\"],axis=1).to_csv(\"Res/Complete_data_processed/enron9.csv\",sep=\",\",index=False)\n",
    "enron10.drop([\"content_entire_mail\",\"content_message\"],axis=1).to_csv(\"Res/Complete_data_processed/enron10.csv\",sep=\",\",index=False)\n",
    "enron11.drop([\"content_entire_mail\",\"content_message\"],axis=1).to_csv(\"Res/Complete_data_processed/enron11.csv\",sep=\",\",index=False)\n",
    "enron12.drop([\"content_entire_mail\",\"content_message\"],axis=1).to_csv(\"Res/Complete_data_processed/enron12.csv\",sep=\",\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
