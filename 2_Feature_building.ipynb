{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building features from the processed data after initial exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "# Chaning back to the resource directory if it is not there\n",
    "if(\"Res\" not in os.getcwd()):\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>mail_label</th>\n",
       "      <th>file_name</th>\n",
       "      <th>date_mail</th>\n",
       "      <th>owner_mail</th>\n",
       "      <th>content_message</th>\n",
       "      <th>subject_message</th>\n",
       "      <th>dataset</th>\n",
       "      <th>line_count</th>\n",
       "      <th>tokens</th>\n",
       "      <th>...</th>\n",
       "      <th>single_char_count</th>\n",
       "      <th>number_token_count</th>\n",
       "      <th>year_count</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>useful_tokens</th>\n",
       "      <th>median_useful_token_len</th>\n",
       "      <th>avg_useful_token_len</th>\n",
       "      <th>rareword_count</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>lemma_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4743</td>\n",
       "      <td>spam</td>\n",
       "      <td>Res/enron1/spam/4743.2005-06-25.GP.spam.txt</td>\n",
       "      <td>2005-06-25 00:00:00</td>\n",
       "      <td>GP</td>\n",
       "      <td>Subject: what up , , your cam babe\\r\\nwhat are...</td>\n",
       "      <td>Subject: what up , , your cam babe</td>\n",
       "      <td>enron1</td>\n",
       "      <td>14</td>\n",
       "      <td>[what, up, ,, ,, your, cam, babe, what, are, y...</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>[cam, babe, looking, looking, companion, frien...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.712500</td>\n",
       "      <td>46.0</td>\n",
       "      <td>[cam, babe, look, look, companion, friendship,...</td>\n",
       "      <td>[cam, babe, looking, looking, companion, frien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1309</td>\n",
       "      <td>spam</td>\n",
       "      <td>Res/enron1/spam/1309.2004-06-08.GP.spam.txt</td>\n",
       "      <td>2004-06-08 00:00:00</td>\n",
       "      <td>GP</td>\n",
       "      <td>Subject: want to make more money ?\\r\\norder co...</td>\n",
       "      <td>Subject: want to make more money ?</td>\n",
       "      <td>enron1</td>\n",
       "      <td>9</td>\n",
       "      <td>[want, to, make, more, money, ?, order, confir...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>[want, make, money, order, confirmation, order...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.081081</td>\n",
       "      <td>8.0</td>\n",
       "      <td>[want, make, money, order, confirm, order, shi...</td>\n",
       "      <td>[want, make, money, order, confirmation, order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>726</td>\n",
       "      <td>spam</td>\n",
       "      <td>Res/enron1/spam/0726.2004-03-26.GP.spam.txt</td>\n",
       "      <td>2004-03-26 00:00:00</td>\n",
       "      <td>GP</td>\n",
       "      <td>Subject: food for thoughts\\r\\n[\\r\\njoin now - ...</td>\n",
       "      <td>Subject: food for thoughts</td>\n",
       "      <td>enron1</td>\n",
       "      <td>6</td>\n",
       "      <td>[food, for, thoughts, [, join, now, -, take, a...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[food, thoughts, join, take, free, tour, click...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[food, thought, join, take, free, tour, click,...</td>\n",
       "      <td>[food, thought, join, take, free, tour, click,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202</td>\n",
       "      <td>spam</td>\n",
       "      <td>Res/enron1/spam/0202.2004-01-13.GP.spam.txt</td>\n",
       "      <td>2004-01-13 00:00:00</td>\n",
       "      <td>GP</td>\n",
       "      <td>Subject: miningnews . net newsletter - tuesday...</td>\n",
       "      <td>Subject: miningnews . net newsletter - tuesday...</td>\n",
       "      <td>enron1</td>\n",
       "      <td>97</td>\n",
       "      <td>[miningnews, ., net, newsletter, -, tuesday, ,...</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>58</td>\n",
       "      <td>17</td>\n",
       "      <td>239.0</td>\n",
       "      <td>[miningnews, net, newsletter, tuesday, january...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.264654</td>\n",
       "      <td>165.0</td>\n",
       "      <td>[miningnew, net, newslett, tuesday, januari, t...</td>\n",
       "      <td>[miningnews, net, newsletter, tuesday, january...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3988</td>\n",
       "      <td>spam</td>\n",
       "      <td>Res/enron1/spam/3988.2005-03-06.GP.spam.txt</td>\n",
       "      <td>2005-03-06 00:00:00</td>\n",
       "      <td>GP</td>\n",
       "      <td>Subject: your pharmacy ta\\r\\nwould you want ch...</td>\n",
       "      <td>Subject: your pharmacy ta</td>\n",
       "      <td>enron1</td>\n",
       "      <td>2</td>\n",
       "      <td>[your, pharmacy, ta, would, you, want, cheap, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[pharmacy, ta, would, want, cheap, perscriptio...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[pharmaci, would, want, cheap, perscript, http...</td>\n",
       "      <td>[pharmacy, would, want, cheap, perscriptions, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id mail_label                                    file_name  \\\n",
       "0  4743       spam  Res/enron1/spam/4743.2005-06-25.GP.spam.txt   \n",
       "1  1309       spam  Res/enron1/spam/1309.2004-06-08.GP.spam.txt   \n",
       "2   726       spam  Res/enron1/spam/0726.2004-03-26.GP.spam.txt   \n",
       "3   202       spam  Res/enron1/spam/0202.2004-01-13.GP.spam.txt   \n",
       "4  3988       spam  Res/enron1/spam/3988.2005-03-06.GP.spam.txt   \n",
       "\n",
       "             date_mail owner_mail  \\\n",
       "0  2005-06-25 00:00:00         GP   \n",
       "1  2004-06-08 00:00:00         GP   \n",
       "2  2004-03-26 00:00:00         GP   \n",
       "3  2004-01-13 00:00:00         GP   \n",
       "4  2005-03-06 00:00:00         GP   \n",
       "\n",
       "                                     content_message  \\\n",
       "0  Subject: what up , , your cam babe\\r\\nwhat are...   \n",
       "1  Subject: want to make more money ?\\r\\norder co...   \n",
       "2  Subject: food for thoughts\\r\\n[\\r\\njoin now - ...   \n",
       "3  Subject: miningnews . net newsletter - tuesday...   \n",
       "4  Subject: your pharmacy ta\\r\\nwould you want ch...   \n",
       "\n",
       "                                     subject_message dataset  line_count  \\\n",
       "0                 Subject: what up , , your cam babe  enron1          14   \n",
       "1                 Subject: want to make more money ?  enron1           9   \n",
       "2                         Subject: food for thoughts  enron1           6   \n",
       "3  Subject: miningnews . net newsletter - tuesday...  enron1          97   \n",
       "4                          Subject: your pharmacy ta  enron1           2   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [what, up, ,, ,, your, cam, babe, what, are, y...   \n",
       "1  [want, to, make, more, money, ?, order, confir...   \n",
       "2  [food, for, thoughts, [, join, now, -, take, a...   \n",
       "3  [miningnews, ., net, newsletter, -, tuesday, ,...   \n",
       "4  [your, pharmacy, ta, would, you, want, cheap, ...   \n",
       "\n",
       "                         ...                          single_char_count  \\\n",
       "0                        ...                                          4   \n",
       "1                        ...                                          1   \n",
       "2                        ...                                          1   \n",
       "3                        ...                                         33   \n",
       "4                        ...                                          0   \n",
       "\n",
       "   number_token_count  year_count  stopword_count  \\\n",
       "0                   0           0            59.0   \n",
       "1                   0           0            20.0   \n",
       "2                   0           0             6.0   \n",
       "3                  58          17           239.0   \n",
       "4                   0           0             2.0   \n",
       "\n",
       "                                       useful_tokens  median_useful_token_len  \\\n",
       "0  [cam, babe, looking, looking, companion, frien...                      5.0   \n",
       "1  [want, make, money, order, confirmation, order...                      6.0   \n",
       "2  [food, thoughts, join, take, free, tour, click...                      4.0   \n",
       "3  [miningnews, net, newsletter, tuesday, january...                      6.0   \n",
       "4  [pharmacy, ta, would, want, cheap, perscriptio...                      4.5   \n",
       "\n",
       "  avg_useful_token_len  rareword_count  \\\n",
       "0             5.712500            46.0   \n",
       "1             6.081081             8.0   \n",
       "2             5.000000             0.0   \n",
       "3             6.264654           165.0   \n",
       "4             5.200000             5.0   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0  [cam, babe, look, look, companion, friendship,...   \n",
       "1  [want, make, money, order, confirm, order, shi...   \n",
       "2  [food, thought, join, take, free, tour, click,...   \n",
       "3  [miningnew, net, newslett, tuesday, januari, t...   \n",
       "4  [pharmaci, would, want, cheap, perscript, http...   \n",
       "\n",
       "                                        lemma_tokens  \n",
       "0  [cam, babe, looking, looking, companion, frien...  \n",
       "1  [want, make, money, order, confirmation, order...  \n",
       "2  [food, thought, join, take, free, tour, click,...  \n",
       "3  [miningnews, net, newsletter, tuesday, january...  \n",
       "4  [pharmacy, would, want, cheap, perscriptions, ...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the initial data processed from the code initial EDA\n",
    "intial_df = pd.read_csv(\"Res/processed_data/final_spam_ham_df.csv\",sep=\",\",\n",
    "                        converters={\"tokens\": literal_eval,\"useful_tokens\": literal_eval,\n",
    "                                   \"stemmed_tokens\": literal_eval,\"lemma_tokens\": literal_eval})\n",
    "intial_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_useful_tokens = []\n",
    "# Using the lemma tokens as the useful tokens\n",
    "for useful_token in intial_df[\"lemma_tokens\"]:\n",
    "    all_useful_tokens.extend(useful_token)\n",
    "all_useful_tokens = list(map(lambda x: x.lower(), all_useful_tokens))\n",
    "\n",
    "from collections import Counter\n",
    "all_useful_tokens_dict = dict(Counter(all_useful_tokens))\n",
    "\n",
    "# Finding the top 3000 tokens based on frequency of occurance\n",
    "useful_tokens_dict = {}\n",
    "for key,value in all_useful_tokens_dict.iteritems():\n",
    "    if(value>220 and len(key)>2):\n",
    "        useful_tokens_dict[key.lower()] = value\n",
    "\n",
    "with open('Res/useful_tokens_dict.txt','w') as data:\n",
    "    data.write(str(useful_tokens_dict))\n",
    "    \n",
    "def intersect_useful_tokens(lemma_tokens_row):\n",
    "    return (list(set(map(lambda x: x.lower(), lemma_tokens_row))&set(useful_tokens_dict.keys())))\n",
    "intial_df[\"attributes\"] = intial_df.apply(lambda row: intersect_useful_tokens(row[\"lemma_tokens\"]),axis=1)\n",
    "intial_df[\"attributes_len\"] = intial_df.apply(lambda row: float(len(row[\"attributes\"])),axis=1)\n",
    "intial_df[\"corpus\"] = intial_df.apply(lambda row: \" \".join(row[\"attributes\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the tf-idf data frame\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = list(intial_df[\"corpus\"])\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "attributes_df = pd.DataFrame(X.todense(),columns = vectorizer.get_feature_names())\n",
    "intial_df = pd.concat([intial_df,attributes_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the final features data into attributes_df.csv file\n",
    "attributes_hot_encode = intial_df\n",
    "attributes_hot_encode.to_csv(\"Res/processed_data/attributes_df.csv\",sep=\",\",index=False)\n",
    "with open(\"Res/processed_data/attributes_names.txt\", 'w') as f:\n",
    "    for item in vectorizer.get_feature_names():\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed, Number of attributes created = 2938\n"
     ]
    }
   ],
   "source": [
    "print(\"Completed, Number of attributes created = {}\".format(len(vectorizer.get_feature_names())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
